{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOW5qYK7jeQZpanANuutpQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslamcm/Projects/blob/main/Spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwJ5nc6Et4u2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7LO5E_LhVpZ",
        "outputId": "d4dd8b21-786d-4814-849b-b1a363c339e8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8G3HRT9gVWf",
        "outputId": "9a0ecafa-f2cb-4f45-b43f-8e737a8cd35f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ZRhcNCtraX"
      },
      "source": [
        "from nltk.classify.util import apply_features\n",
        "from nltk import NaiveBayesClassifier\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import io\n",
        "import re\n",
        "from google.colab import files\n",
        "import nltk\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FthfKZ4BfnZ-",
        "outputId": "2421896d-5239-47a2-f0a0-962e065ddb5b"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eee0eacd-aa6b-4132-a7a4-ea12b4f372e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eee0eacd-aa6b-4132-a7a4-ea12b4f372e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving emails_1.csv to emails_1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfXaHOj9RSE",
        "outputId": "60381de0-1197-44a2-8233-1390059c3961"
      },
      "source": [
        "df = pd.read  _csv('emails_1.csv')\n",
        "print(df)\n",
        "df.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            text,spam\n",
            "0   \"Subject: this free 7 - day trial will prove t...\n",
            "1   \"Subject: followup from iris mack  hi ,  thank...\n",
            "2   \"Subject: make your rivals envy  lt is really ...\n",
            "3   \"Subject: re : telephone interview with the en...\n",
            "4   \"Subject: a 1 time charge add your property / ...\n",
            "..                                                ...\n",
            "95  \"Subject: the garp 2001 convention  dear garp ...\n",
            "96  \"Subject: the penis patch is amazing  the peni...\n",
            "97  \"Subject: dave d . presentation  vince ,  here...\n",
            "98  \"Subject: re : chicago partners  david ,  i se...\n",
            "99  \"Subject: levltrra , xana , merldll  hello , t...\n",
            "\n",
            "[100 rows x 1 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 1 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   text,spam  100 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 928.0+ bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMJ72WM8edjw",
        "outputId": "a2670411-5cb5-4967-f016-efc5c8ff5ec5"
      },
      "source": [
        "import re\n",
        "doc = \"NLP                ,               is an interesting     field.  \"\n",
        "new_doc = re.sub(\" +\",\" \", doc)\n",
        "print(new_doc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLP , is an interesting field. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HJrm94b9fH9A",
        "outputId": "a9888e15-1ec4-4db6-aa07-3edde160642e"
      },
      "source": [
        "text = \"Hello! How are you!! I'm very excited that you're going for a 9trip to Europe!! Yayy!\"\n",
        "re.sub(\"[^A-Za-z ]\", \"\" , text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello How are you Im very excited that youre going for a trip to Europe Yayy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78JejOFhfOvn",
        "outputId": "e583168c-de4a-4002-d79f-33e4dcc32f56"
      },
      "source": [
        "import string\n",
        "text = \"Hello             exciting        ! europe  How are you he went and gone to today!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "\n",
        "text_clean = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
        "text_clean = re.sub(\" +\",\" \", text_clean)\n",
        "text_clean = re.sub(\"[^A-Za-z ]\", \"\" , text_clean)\n",
        "text_clean\n",
        "tokenized_words = nltk.tokenize.word_tokenize(text_clean)\n",
        "tokenized_words\n",
        "words_new = [i for i in tokenized_words if i not in stopwords and len(i) >2]\n",
        "print(words_new)\n",
        "\n",
        "ps = nltk.PorterStemmer()\n",
        "w = [ps.stem(word) for word in words_new]\n",
        "print(w)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'exciting', 'europe', 'went', 'gone', 'today', 'excited', 'youre', 'going', 'trip', 'europe', 'yayy']\n",
            "['hello', 'excit', 'europ', 'went', 'gone', 'today', 'excit', 'your', 'go', 'trip', 'europ', 'yayi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzjViAuzfht5"
      },
      "source": [
        "text = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "nltk.tokenize.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isIxni1ig78k",
        "outputId": "81a9e477-bb72-4a57-d97d-57fe03948f12"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "text = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
        "text_new = \"\".join([i for i in text if i not in string.punctuation])\n",
        "print(text_new)\n",
        "words = nltk.tokenize.word_tokenize(text_new)\n",
        "print(words)\n",
        "words_new = [i for i in words if i not in stopwords]\n",
        "print(words_new)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello How are you Im very excited that youre going for a trip to Europe Yayy\n",
            "['Hello', 'How', 'are', 'you', 'Im', 'very', 'excited', 'that', 'youre', 'going', 'for', 'a', 'trip', 'to', 'Europe', 'Yayy']\n",
            "['Hello', 'How', 'Im', 'excited', 'youre', 'going', 'trip', 'Europe', 'Yayy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ki5IqRi_eh",
        "outputId": "587a9aa5-565a-4415-cea1-6018139bfedd"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "w = [ps.stem(word) for word in words_new]\n",
        "print(w)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'how', 'Im', 'excit', 'your', 'go', 'trip', 'europ', 'yayi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOxpIv0MjQ7S",
        "outputId": "6d32c9d3-fa5a-47b7-a3c5-7cf9c9c3399e"
      },
      "source": [
        "wn = nltk.WordNetLemmatizer()\n",
        "w2 = [wn.lemmatize(word) for word in words_new]\n",
        "print(w2)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'How', 'Im', 'excited', 'youre', 'going', 'trip', 'Europe', 'Yayy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdvKhLx2gLFq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "smJcg2x4vi3T",
        "outputId": "2aa6640e-1ee4-4e24-872a-7956dce8cc64"
      },
      "source": [
        "\n",
        "test_str = \"Subject: this free 7 - day trial will prove that you can get ready for the beach .  if you wish to unsubscribe click  here  or write to ultima group , inc . 1380 garnet avenue , e 520 san diego , ca 92109  nbtnsbpa\",1\n",
        "#res = tuple(map(int, test_str.split(', ')))\n",
        "\n",
        "#test_str = str(list((test_str)))\n",
        "type(test_str)\n",
        "test_str[0]\n",
        "#res = tuple(test_str.split (\",\"))\n",
        "#res"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Subject: this free 7 - day trial will prove that you can get ready for the beach .  if you wish to unsubscribe click  here  or write to ultima group , inc . 1380 garnet avenue , e 520 san diego , ca 92109  nbtnsbpa'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFbeZjC-4SKv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNgWFesFtxOR"
      },
      "source": [
        "from nltk.classify.util import apply_features\n",
        "from nltk import NaiveBayesClassifier\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from nltk.probability import DictionaryProbDist\n",
        "\n",
        "class SpamClassifier:\n",
        "\n",
        "    def getTokens(self, text):\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "        ps = nltk.PorterStemmer()\n",
        "\n",
        "        text_clean = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
        "        text_clean = re.sub(\" +\",\" \", text_clean)\n",
        "        text_clean = re.sub(\"[^A-Za-z ]\", \"\" , text_clean)\n",
        "        tokenized_words = nltk.tokenize.word_tokenize(text_clean)\n",
        "        words_new = [i for i in tokenized_words if i not in stopwords and len(i) >2]\n",
        "        return [ps.stem(word) for word in words_new]\n",
        "    \n",
        "    def extract_tokens(self, text, target):\n",
        "        \"\"\"returns array of tuples where each tuple is defined by (tokenized_text, label)\n",
        "         parameters:\n",
        "                text: array of texts\n",
        "                target: array of target labels\n",
        "                \n",
        "        NOTE: consider only those words which have all alphabets and atleast 3 characters.\n",
        "        \"\"\"\n",
        "        corpus=[]\n",
        "        \n",
        "        for (single_text, single_target) in zip(text, target):\n",
        "          corpus.append((self.getTokens(single_text), single_target))\n",
        "        \n",
        "        return corpus\n",
        "        \n",
        "    \n",
        "    def get_features(self, corpus):\n",
        "        \"\"\" \n",
        "        returns a Set of unique words in complete corpus. \n",
        "        parameters:- corpus: tokenized corpus along with target labels (i.e)the ouput of extract_tokens function.\n",
        "        \n",
        "        Return Type is a set\n",
        "        \"\"\"\n",
        "        featureSet = set()\n",
        "        for pair in corpus:\n",
        "          for word in pair[0]:\n",
        "            featureSet.add(word)\n",
        "        return featureSet\n",
        "\n",
        "    \n",
        "    def extract_features(self, document):\n",
        "        \"\"\"\n",
        "        maps each input text into feature vector\n",
        "        parameters:- document: string\n",
        "        \n",
        "        Return type : A dictionary with keys being the train dataset word features.\n",
        "                      The values correspond to True or False\n",
        "        \"\"\"\n",
        "\t\tfeatures={}\n",
        "\t\tdoc_words = set(document)\n",
        "\t\t#iterate through the word_features to find if the doc_words contains it or not\n",
        "\t\tfor word in word_features:\n",
        "      features[word] = word in self.doc_words\n",
        "\t\t\n",
        "\t\treturn features\n",
        "        \n",
        "    def getLabelWordProbabilities(train_set, label_set):\n",
        "      fin_dict = {}\n",
        "      for word in self.word_features:\n",
        "        for label in label_set:\n",
        "          true_count = 0\n",
        "          false_count = 0\n",
        "          for t_set in train_set:\n",
        "            if t_set[1] is not label:\n",
        "              continue\n",
        "            if t_set[0][word] is True:\n",
        "              true_count += 1\n",
        "            else:\n",
        "              false_count += 1\n",
        "          fin_dict[(label, word)] = DictionaryProbDist({True: true_count / (false_count + true_count)})\n",
        "      return fin_dict\n",
        "\n",
        "    def train(self, text, labels):\n",
        "        \"\"\"\n",
        "        Returns trained model and set of unique words in training data\n",
        "        also set trained model to 'self.classifier' variable and set of \n",
        "        unique words to 'self.word_features' variable.\n",
        "        \"\"\"\n",
        "\t\t#call extract_tokens\n",
        "\t\tself.corpus=self.extract_tokens(text, labels)\n",
        "\t\t\n",
        "\t\t#call get_features\n",
        "\t\tself.word_features=self.get_features(self.corpus)\n",
        "\t\t\n",
        "\t\t#Extracting training set\n",
        "\t\ttrain_set = apply_features(self.extract_features, self.corpus)\n",
        "\t\t\n",
        "\t\t#Now train the NaiveBayesClassifier with train_set\n",
        "\n",
        "    one_prob = labels.count(1) / len(labels)\n",
        "    label_probabilities = DictionaryProbDist({1: one_prob, 0: 1-one_prob})\n",
        "    label_word_probabilities = self.getLabelWordProbabilities(train_set, set(labels))\n",
        "\n",
        "        self.classifier = NaiveBayesClassifier(label_probabilities, label_word_probabilities)\n",
        "\t\t\n",
        "        return self.classifier, self.word_features\n",
        "\n",
        "\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "        \n",
        "    \n",
        "    def predict(self, text):\n",
        "        \"\"\"\n",
        "        Returns prediction labels of given input text. \n",
        "        Allowed Text can be a simple string i.e one input email, a list of emails, or a dictionary of emails identified by their labels.\n",
        "        \"\"\"\n",
        "\t\tif isinstance(text, (list)):\n",
        "            pred = []\n",
        "            for sentence in list(text):\n",
        "                pred.append(self.classifier.classify(self.extract_features(sentence.split())))\n",
        "            return pred\n",
        "        if isinstance(text, (collections.OrderedDict)):\n",
        "            pred = collections.OrderedDict()\n",
        "            for label, sentence in text.items():\n",
        "                pred[label] = self.classifier.classify(self.extract_features(sentence.split()))\n",
        "            return pred\n",
        "        return self.classifier.classify(self.extract_features(text.split()))\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    data = pd.read_csv('emails.csv')\n",
        "    train_X, test_X, train_Y, test_Y = train_test_split(data[\"text\"].values,\n",
        "                                                            data[\"spam\"].values,\n",
        "                                                            test_size = 0.25,\n",
        "                                                            random_state = 50,\n",
        "                                                            shuffle = True,\n",
        "                                                            stratify=data[\"spam\"].values)\n",
        "    classifier = SpamClassifier()\n",
        "    classifier_model, model_word_features = classifier.train(train_X, train_Y)\n",
        "    model_name = 'spam_classifier_model.pk'\n",
        "    model_word_features_name = 'spam_classifier_model_word_features.pk'\n",
        "    with open(model_name, 'wb') as  model_fp:\n",
        "        pickle.dump(classifier_model, model_fp)\n",
        "    with open(model_word_features_name, 'wb') as model_fp:\n",
        "            pickle.dump(model_word_features, model_fp)\n",
        "    print('DONE')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OppFwJn2t1Tr"
      },
      "source": [
        "# mohsin code\n",
        "\n",
        "from nltk.classify.util import apply_features\n",
        "from nltk import NaiveBayesClassifier\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import re\n",
        "from nltk.probability import DictionaryProbDist\n",
        "\n",
        "class SpamClassifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('wordnet')\n",
        "        self.wn = nltk.WordNetLemmatizer()\n",
        "        self.ps = nltk.PorterStemmer()\n",
        "\n",
        "    def getTokens(self, text):\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "        text_clean = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
        "        text_clean = re.sub(\" +\",\" \", text_clean)\n",
        "        text_clean = re.sub(\"[^A-Za-z ]\", \"\" , text_clean)\n",
        "        tokenized_words = nltk.tokenize.word_tokenize(text_clean)\n",
        "        words_new = [i for i in tokenized_words if i not in stopwords and len(i) >2 and i.isalpha()]\n",
        "        return set([self.wn.lemmatize(word) for word in words_new])\n",
        "    \n",
        "    def extract_tokens(self, text, target):\n",
        "        \"\"\"returns array of tuples where each tuple is defined by (tokenized_text, label)\n",
        "         parameters:\n",
        "                text: array of texts\n",
        "                target: array of target labels\n",
        "                \n",
        "        NOTE: consider only those words which have all alphabets and atleast 3 characters.\n",
        "        \"\"\"\n",
        "        corpus=[]\n",
        "        \n",
        "        for (single_text, single_target) in zip(text, target):\n",
        "            corpus.append((self.getTokens(single_text), single_target))\n",
        "        \n",
        "        return corpus\n",
        "        \n",
        "    \n",
        "    def get_features(self, corpus):\n",
        "        \"\"\" \n",
        "        returns a Set of unique words in complete corpus. \n",
        "        parameters:- corpus: tokenized corpus along with target labels (i.e)the ouput of extract_tokens function.\n",
        "        \n",
        "        Return Type is a set\n",
        "        \"\"\"\n",
        "        featureSet = set()\n",
        "        for pair in corpus:\n",
        "            for word in pair[0]:\n",
        "                featureSet.add(word)\n",
        "        return featureSet\n",
        "\n",
        "    \n",
        "    def extract_features(self, document):\n",
        "        \"\"\"\n",
        "        maps each input text into feature vector\n",
        "        parameters:- document: string\n",
        "        \n",
        "        Return type : A dictionary with keys being the train dataset word features.\n",
        "                      The values correspond to True or False\n",
        "        \"\"\"\n",
        "        features={}\n",
        "        doc_words = set(document)\n",
        "        #iterate through the word_features to find if the doc_words contains it or not\n",
        "        for word in self.word_features:\n",
        "            features[word] = word in doc_words\n",
        "        \n",
        "        return features\n",
        "        \n",
        "    def getLabelWordProbabilities(self, train_set, label_set):\n",
        "        fin_dict = {}\n",
        "        temp_dict = {}\n",
        "\n",
        "        for t_set in train_set:\n",
        "            label = t_set[1]\n",
        "            for word in t_set[0]:\n",
        "                state = t_set[0][word]\n",
        "                if (label, word, state) not in temp_dict:\n",
        "                  temp_dict[(label, word, state)] = 0\n",
        "                temp_dict[(label, word, state)] += 1\n",
        "        for word in self.word_features:\n",
        "            for label in label_set:\n",
        "                true_count = 0\n",
        "                if (label, word, True) in temp_dict:\n",
        "                  true_count = temp_dict[(label, word, True)]\n",
        "                false_count = 0\n",
        "                if (label, word, False) in temp_dict:\n",
        "                  false_count = temp_dict[(label, word, False)]\n",
        "                fin_dict[(label, word)] = DictionaryProbDist({True: true_count / (true_count + false_count)})\n",
        "        return fin_dict\n",
        "\n",
        "    def train(self, text, labels):\n",
        "        \"\"\"\n",
        "        Returns trained model and set of unique words in training data\n",
        "        also set trained model to 'self.classifier' variable and set of \n",
        "        unique words to 'self.word_features' variable.\n",
        "        \"\"\"\n",
        "        #call extract_tokens\n",
        "        self.corpus=self.extract_tokens(text, labels)\n",
        "        \n",
        "        #call get_features\n",
        "        self.word_features=self.get_features(self.corpus)\n",
        "        \n",
        "        #Extracting training set\n",
        "        train_set = apply_features(self.extract_features, self.corpus)\n",
        "        \n",
        "        #Now train the NaiveBayesClassifier with train_set\n",
        "        one_prob = (labels == 1).sum() / len(labels)\n",
        "        label_probabilities = DictionaryProbDist({1: one_prob, 0: 1-one_prob})\n",
        "        label_word_probabilities = self.getLabelWordProbabilities(train_set, set(labels))\n",
        "        print(label_probabilities)\n",
        "        self.classifier = NaiveBayesClassifier(label_probabilities, label_word_probabilities)\n",
        "        return self.classifier, self.word_features\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"\n",
        "        Returns prediction labels of given input text. \n",
        "        Allowed Text can be a simple string i.e one input email, a list of emails, or a dictionary of emails identified by their labels.\n",
        "        \"\"\"\n",
        "        if isinstance(text, (list)):\n",
        "            pred = []\n",
        "            for sentence in list(text):\n",
        "                pred.append(self.classifier.classify(self.extract_features(sentence.split())))\n",
        "            return pred\n",
        "        if isinstance(text, (collections.OrderedDict)):\n",
        "            pred = collections.OrderedDict()\n",
        "            for label, sentence in text.items():\n",
        "                pred[label] = self.classifier.classify(self.extract_features(sentence.split()))\n",
        "            return pred\n",
        "        return self.classifier.classify(self.extract_features(text.split()))\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    data = pd.read_csv('emails.csv')\n",
        "    train_X, test_X, train_Y, test_Y = train_test_split(data[\"text\"].values,\n",
        "                                                            data[\"spam\"].values,\n",
        "                                                            test_size = 0.25,\n",
        "                                                            random_state = 50,\n",
        "                                                            shuffle = True,\n",
        "                                                            stratify=data[\"spam\"].values)\n",
        "    classifier = SpamClassifier()\n",
        "    classifier_model, model_word_features = classifier.train(train_X, train_Y)\n",
        "    model_name = 'spam_classifier_model.pk'\n",
        "    model_word_features_name = 'spam_classifier_model_word_features.pk'\n",
        "    with open(model_name, 'wb') as  model_fp:\n",
        "        pickle.dump(classifier_model, model_fp)\n",
        "    with open(model_word_features_name, 'wb') as model_fp:\n",
        "            pickle.dump(model_word_features, model_fp)\n",
        "    print('DONE')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}